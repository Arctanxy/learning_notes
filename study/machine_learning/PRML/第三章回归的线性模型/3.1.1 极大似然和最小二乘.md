# 极大似然和最小二乘

本文为PRML第三章3.1.1笔记，书中推导步骤过于简单，对长期没有接触高数的读者很不友好，故在此记录一下由极大似然到最小二乘的详细的推导过程。

线性回归的方程可以写成如下形式：

$$y = f(x,w) + \epsilon$$

其中$\epsilon$是一个均值为零的高斯随机变量，精度（方差的倒数）为$\beta$。

可以认为$y$值符合均值为$f(x,w)$，方差为$\beta^{-1}$的高斯分布：

$$p(y|x,w,\beta) = \mathcal N(y|f(x,w),\beta^{-1})$$

根据高斯分布的概率分布公式可以转化为：

$$f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}

 = \sqrt{\frac{\beta}{2\pi}} e^{-\frac{\beta(x-\mu)^2}{2}}$$

 $$\mathcal N(y|f(x,w),\beta^{-1}) = \sqrt{\frac{\beta}{2\pi}} e^{-\frac{\beta(y-f(x,w))^2}{2}}$$


在此条件分布下，可以很快地求得$y$的条件均值：

$$E[y|x] = \int yp(y|x)dy = f(x,w)$$

在实际问题中，$X$为数据集$\{x_1,...,x_n\}$，对应的目标值$y_1,...y_n$，$\{y_n\}$是一个列向量，记作$y$，可以得到如下公式：

$$p(y|X,w,\beta) = \prod_{n=1}^{N} \mathcal N(y_n|w^T\phi(x_n),\beta^{-1})$$

对两边同时取导数

$$ln p(y|X,w,\beta) = \sum_{n=1}^{N}ln \mathcal N(y_n|w^T\phi(x_n),\beta^{-1})$$

将前面的概率公式带入之后可以写成

$$ln p(y|X,w,\beta) = \sum_{n=1}^{N}ln \{\sqrt{\frac{\beta}{2\pi}} e^{-\frac{\beta(y-f(x,w))^2}{2}}\}$$

$$ln p(y|X,w,\beta) = \sum_{n=1}^{N} \{\frac{ln \beta}{2} - \frac{ln2\pi}{2} - \frac{\beta (y_n-f(x,w))^2}{2}\}$$

$$ln p(y|X,w,\beta) = \frac{Nln \beta}{2} - \frac{Nln2\pi}{2} -\sum_{n=1}^{N} \frac{\beta (y_n-f(x,w))^2}{2}$$

为了使概率$p(y|X,w,\beta)$最大，就需要使$\sum_{n=1}^{N} \frac{\beta (y_n-f(x,w))^2}{2}$最小，也就是使$\sum_{n=1}^{N} (y_n-f(x,w))^2$最小。

如此一来，又变成了一个最小二乘问题，对其求导后令导数为零，即可得到最终$w$的解(具体推导过程参考[线性回归与岭回归的数学推导](https://arctanxy.github.io/2018/04/05/%E5%B2%AD%E5%9B%9E%E5%BD%92/))：

$$w = (X^TX)^{-1}y^T X$$
