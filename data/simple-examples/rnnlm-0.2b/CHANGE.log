0.1g:
- classes at output layer are used to limit computational cost of normalization; speedup ~50x with only small performance loss!
- discarded -vocab-threshold, -compression

0.1h:
- loops rewritten; ~20% better performance
- blas option removed
- changed output during training
- progress of training is shown (switch -debug 2)
- added '-beta' parameter for setting L2 regularization parameter
- added '-one-iter' parameter: performs one iteration over training data and saves the resulting model; does not use valid data
- specifying '-alpha' as a parameter now overrides value found in the model file; can be combined with '-one-iter' to perform adaptation on new data
- added '-min-improvement' parameter to control speed of training process (can be turned off by specifying '-min improvement 1.0')
- added '-anti-kasparek' option to allow more stability during training in unstable environments (saves model during iterations after processing specified amount of words)
- improved bptt implementation, more than 3x speed for large bptt values now (controlled by bptt_block defined in rnnlmlib.cpp)

0.2a:
- improved BPTT implementation, added '-bptt-block N' switch so that error is propagated after N examples, which results to BPTT step + N complexity
  of training instead of BPTT step * N

0.2b:
- added '-direct N' parameter to allow direct connections between top N words at input and output layers + between all input words and
  all classes at output layer (for values >0)
