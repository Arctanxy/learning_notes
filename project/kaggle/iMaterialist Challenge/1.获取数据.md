# 获取数据

赛事组织方只提供了所有图片的地址，而没有把图片打包提供给我们下载，这样就需要写个小爬虫，从这些地址中爬取图片。

需要爬取的数据量很大，据说有30G大小，而且部分图片还在长城之外，所以第一步下载图片就是个麻烦活，可以说在国内很难靠自己获取到全部数据集。

程序给我返回的信息都是这样的：

```python
HTTP Error 403: Forbidden
HTTP Error 403: Forbidden
HTTP Error 403: Forbidden
<urlopen error [Errno 11004] getaddrinfo failed>
[WinError 10054] 远程主机强迫关闭了一个现有的连接。
HTTP Error 503: Service Temporarily Unavailable
HTTP Error 403: Forbidden
HTTP Error 502: Bad Gateway
HTTP Error 404: Not Found
HTTP Error 403: Forbidden
<urlopen error [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。>
<urlopen error [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。>
HTTP Error 503: Service Temporarily Unavailable
```

所以项目后面如果出现了图片缺失的话，我们只能分析分析图片的url字符中包含了什么信息，也许能弥补一点正确率。

## 1. 处理json文件

将json格式的数据转换为dataframe格式的数据，在下载和建模过程中，dataframe格式的数据遍历起来会方便很多。


## 2. 下载

因为数据量大，几乎不可能一次性下完，所以要考虑断点下载，即下载之前检查该图片是否已经存在于文件夹中，如果存在则跳过，不存在则下载。

实不相瞒，我最近每天晚上回去都会开着爬虫下载，但目前只下载了不到一万张图片。

所以我下载这些图片只是积累一点图像识别的素材，学习学习图像分类算法。

下载代码如下，有梯子的朋友架上梯子会好很多：

```Python
import urllib.request
import json
import pandas as pd 
from tqdm import tqdm
import threading
import time
import os

FOLDER = 'H:/learning_notes/project/kaggle/iMaterialist Challenge/'
DOWNLOAD_PATH = 'H:/google_image/'

def download_data(is_type = 'train'):
    time.sleep(0.5)
    images = pd.read_json(FOLDER + is_type + '.json')
    images['url'] = images['images'].apply(lambda x:x['url'][0])
    images['name'] = images['url'].apply(lambda x:x.split('/')[-1])#按网址命名
    images['id'] = images['images'].apply(lambda x:x['image_id'])
    del images['images']
    if is_type == 'train':
        images['label'] = images['annotations'].apply(lambda x:x['label_id'])#图片分类标签
        del images['annotations']
        #images.to_csv(FOLDER + is_type + '_data.csv',index = False)#已经保存过了
        print('total num:%d' % images.shape[0])
        for i,row in images.iterrows():
            if os.path.exists(DOWNLOAD_PATH + is_type + '_image/' + row['name']):#如果这个文件已经存在了，就不需要重复下载了
                continue       
            if i%10 == 0:
                print('train %d' % i)
            try:
                urllib.request.urlretrieve(row['url'],filename=DOWNLOAD_PATH + is_type + '_image/' + row['name'])
            except Exception as e:
                print(e)
    elif is_type == 'validation':
        images['label'] = images['annotations'].apply(lambda x:x['label_id'])#图片分类标签
        del images['annotations']
        #images.to_csv(FOLDER + is_type + '_data.csv',index = False)#已经保存过了
        print('total num:%d' % images.shape[0])
        for i,row in images.iterrows():
            if os.path.exists(DOWNLOAD_PATH + is_type + '_image/' + row['name']):
                continue 
            if i%10 == 0:
                print('validation %d' % i)
            try:
                urllib.request.urlretrieve(row['url'],filename=DOWNLOAD_PATH + is_type + '_image/' + row['name'])
            except Exception as e:
                print(e)
    else:
        print('total num:%d' % images.shape[0])
        for i,row in images.iterrows():
            if os.path.exists(DOWNLOAD_PATH + is_type + '_image/' + row['name']):
                continue 
            if i%10 == 0:
                print('test %d' % i)
            try:
                urllib.request.urlretrieve(row['url'],filename=DOWNLOAD_PATH + is_type + '_image/' + row['name'])
            except Exception as e:
                print(e)
    
if __name__ == "__main__":
    threads = []
    for item in ['test','train','validation']:
        threads.append(threading.Thread(target=download_data,args=(item,)))
    for t in threads:
        t.start()

```